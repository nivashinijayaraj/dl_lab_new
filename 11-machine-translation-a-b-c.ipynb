{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1926230,"sourceType":"datasetVersion","datasetId":1148896},{"sourceId":9658933,"sourceType":"datasetVersion","datasetId":5900884}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"A.Basic Machine Translation using Rule -Based Methods ","metadata":{}},{"cell_type":"code","source":"# Step 1: Define the Bilingual Dictionary\ndictionary = {\n    'hello': 'bonjour',\n    'world': 'monde',\n    'my': 'mon',\n    'name': 'nom',\n    'is': 'est',\n    'good': 'bon',\n    'morning': 'matin',\n    'i': 'je',\n    'am': 'suis',\n    'a': 'un',\n    'student': 'étudiant',\n    'teacher': 'professeur'\n}\n\n# Step 2: Define Grammar Rules\ngrammar_rules = {\n    'SVO': ['subject', 'verb', 'object']  # Subject-Verb-Object structure\n}\n\n# Step 3: Translation Function\ndef translate(sentence):\n    # Convert sentence to lowercase and split into words\n    words = sentence.lower().split()\n    \n    # Translate each word using the dictionary\n    translated_words = [dictionary.get(word, word) for word in words]\n    \n    # Join the translated words into a sentence\n    translated_sentence = ' '.join(translated_words)\n    \n    return translated_sentence\n\n# Example usage\nsentence = \"Hello world\"\ntranslated_sentence = translate(sentence)\nprint(\"Translated sentence:\", translated_sentence)\n\n# Sample input/output interaction\nwhile True:\n    user_input = input(\"Enter an English sentence to translate (or type 'exit' to quit): \")\n    if user_input.lower() in ['exit', 'quit']:\n        print(\"Exiting translation system.\")\n        break\n    \n    translated_output = translate(user_input)\n    print(\"Translated sentence:\", translated_output)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-18T18:46:21.636560Z","iopub.execute_input":"2024-10-18T18:46:21.636987Z","iopub.status.idle":"2024-10-18T18:47:05.514862Z","shell.execute_reply.started":"2024-10-18T18:46:21.636946Z","shell.execute_reply":"2024-10-18T18:47:05.513745Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Translated sentence: bonjour monde\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter an English sentence to translate (or type 'exit' to quit):  hi\n"},{"name":"stdout","text":"Translated sentence: hi\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter an English sentence to translate (or type 'exit' to quit):  how are you\n"},{"name":"stdout","text":"Translated sentence: how are you\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter an English sentence to translate (or type 'exit' to quit):  hello \n"},{"name":"stdout","text":"Translated sentence: bonjour\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter an English sentence to translate (or type 'exit' to quit):  my\n"},{"name":"stdout","text":"Translated sentence: mon\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter an English sentence to translate (or type 'exit' to quit):  exit\n"},{"name":"stdout","text":"Exiting translation system.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"B. English to French Translation using Seq2Seq with Attention ","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport pandas as pd\nimport tensorflow_datasets as tfds\n\n# Step 1: Load dataset from CSV using Pandas\n\ndata = pd.read_csv('/kaggle/input/wmt-dataset-en-fr/wmt14_translate_fr-en_test.csv')\n\n# Check the first few rows and the column names of the dataframe\nprint(data.head())\nprint(\"Columns in the DataFrame:\", data.columns.tolist())  # Print the actual column names\n\n# Ensure the dataframe contains the required columns\nexpected_columns = ['en', 'fr']\nassert all(col in data.columns for col in expected_columns), f\"CSV must contain {expected_columns} columns\"\n\n# Step 1.1: Preprocess the data\n# Drop rows with missing or non-string values\ndata = data.dropna(subset=['en', 'fr'])  # Drop rows where 'en' or 'fr' is NaN\ndata['en'] = data['en'].astype(str)  # Ensure 'en' column is of type string\ndata['fr'] = data['fr'].astype(str)  # Ensure 'fr' column is of type string\n\n# Step 2: Convert the DataFrame to a TensorFlow Dataset\ntrain_dataset = tf.data.Dataset.from_tensor_slices((data['en'].values, data['fr'].values))\n\n# Print the first example to verify conversion\nfor english, french in train_dataset.take(1):\n    print(f'English: {english.numpy().decode(\"utf-8\")}, French: {french.numpy().decode(\"utf-8\")}')\n\n# Optional: Define constants for batch size and max length\nBATCH_SIZE = 64\nMAX_LENGTH = 40\n\n# Optional: Tokenization process\ntokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n    (en.numpy() for en, fr in train_dataset), target_vocab_size=2**13)\ntokenizer_fr = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n    (fr.numpy() for en, fr in train_dataset), target_vocab_size=2**13)\n\n# Encoding function\ndef encode(en_t, fr_t):\n    en_t = [tokenizer_en.vocab_size] + tokenizer_en.encode(en_t.numpy().decode('utf-8')) + [tokenizer_en.vocab_size + 1]\n    fr_t = [tokenizer_fr.vocab_size] + tokenizer_fr.encode(fr_t.numpy().decode('utf-8')) + [tokenizer_fr.vocab_size + 1]\n    return en_t, fr_t\n\ndef tf_encode(en_t, fr_t):\n    return tf.py_function(encode, [en_t, fr_t], [tf.int64, tf.int64])\n\n# Prepare the dataset with encoding\ntrain_dataset = train_dataset.map(tf_encode)\n\n# Filter sequences longer than MAX_LENGTH\ndef filter_max_length(en, fr, max_length=MAX_LENGTH):\n    return tf.logical_and(tf.size(en) <= max_length, tf.size(fr) <= max_length)\n\ntrain_dataset = train_dataset.filter(lambda en, fr: filter_max_length(en, fr, MAX_LENGTH))\n\n# Shuffle and batch the dataset\ntrain_dataset = train_dataset.shuffle(20000).padded_batch(BATCH_SIZE, padded_shapes=([None], [None]))\ntrain_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n\n# Print the first training example after processing\nfor en, fr in train_dataset.take(1):\n    print(f'Encoded English: {en.numpy()}')\n    print(f'Encoded French: {fr.numpy()}')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-18T11:14:55.997737Z","iopub.execute_input":"2024-10-18T11:14:55.998741Z","iopub.status.idle":"2024-10-18T11:16:21.134319Z","shell.execute_reply.started":"2024-10-18T11:14:55.998693Z","shell.execute_reply":"2024-10-18T11:16:21.133041Z"},"trusted":true},"outputs":[{"name":"stdout","text":"                                                  en  \\\n0              Spectacular Wingsuit Jump Over Bogota   \n1  Sportsman Jhonathan Florez jumped from a helic...   \n2  Wearing a wingsuit, he flew past over the famo...   \n3                           A black box in your car?   \n4  As America's road planners struggle to find th...   \n\n                                                  fr  \n0  Spectaculaire saut en \"wingsuit\" au-dessus de ...  \n1  Le sportif Jhonathan Florez a sauté jeudi d'un...  \n2  Equipé d'un wingsuit (une combinaison munie d'...  \n3               Une boîte noire dans votre voiture ?  \n4  Alors que les planificateurs du réseau routier...  \nColumns in the DataFrame: ['en', 'fr']\nEnglish: Spectacular Wingsuit Jump Over Bogota, French: Spectaculaire saut en \"wingsuit\" au-dessus de Bogota\nEncoded English: [[7639 7417 7151 ...    0    0    0]\n [7639 3171  430 ...    0    0    0]\n [7639   12 1233 ...    0    0    0]\n ...\n [7639  820  112 ...    0    0    0]\n [7639   31 4133 ...    0    0    0]\n [7639  531    1 ...    0    0    0]]\nEncoded French: [[8256 1526    5 ...    0    0    0]\n [8256 4802 1244 ...    0    0    0]\n [8256   31 1454 ...    0    0    0]\n ...\n [8256 3149 3786 ...    0    0    0]\n [8256   99 5428 ...    0    0    0]\n [8256  278 8108 ...    0    0    0]]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"C. Neural Machine Translation with Transformers (English to Germans)","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_datasets as tfds\nimport tensorflow_text as text\n\n# Load the dataset\ndataset, metadata = tfds.load('wmt14_translate/de-en', with_info=True, as_supervised=True)\ntrain_examples, val_examples = dataset['train'], dataset['validation']\n\n# Tokenizer and preprocessing\ntokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n    (en.numpy() for en, de in train_examples), target_vocab_size=2**13)\ntokenizer_de = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n    (de.numpy() for en, de in train_examples), target_vocab_size=2**13)\n\ndef encode(lang1, lang2):\n    lang1 = [tokenizer_en.vocab_size] + tokenizer_en.encode(lang1.numpy()) + [tokenizer_en.vocab_size+1]\n    lang2 = [tokenizer_de.vocab_size] + tokenizer_de.encode(lang2.numpy()) + [tokenizer_de.vocab_size+1]\n    return lang1, lang2\n\ndef tf_encode(en, de):\n    result_en, result_de = tf.py_function(encode, [en, de], [tf.int64, tf.int64])\n    result_en.set_shape([None])\n    result_de.set_shape([None])\n    return result_en, result_de\n\ntrain_dataset = train_examples.map(tf_encode).padded_batch(64, padded_shapes=([None], [None]))\nval_dataset = val_examples.map(tf_encode).padded_batch(64, padded_shapes=([None], [None]))\n\n# Transformer Model\nclass Transformer(tf.keras.Model):\n    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n        super(Transformer, self).__init__()\n        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n    \n    def call(self, inputs, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n        enc_output = self.encoder(inputs[0], training, enc_padding_mask)\n        dec_output, _ = self.decoder(inputs[1], enc_output, training, look_ahead_mask, dec_padding_mask)\n        final_output = self.final_layer(dec_output)\n        return final_output\n\n# Define hyperparameters\nnum_layers = 4\nd_model = 128\ndff = 512\nnum_heads = 8\ndropout_rate = 0.1\n\ninput_vocab_size = tokenizer_en.vocab_size + 2\ntarget_vocab_size = tokenizer_de.vocab_size + 2\npe_input = 1000\npe_target = 1000\n\n# Instantiate and compile model\ntransformer = Transformer(num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, dropout_rate)\n\nlearning_rate = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.0001, decay_steps=10000, decay_rate=0.9)\noptimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n\ntransformer.compile(optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n\n# Training loop\ntransformer.fit(train_dataset, epochs=10, validation_data=val_dataset)\n\n# Sample translation\nsample_sentence = \"Hello, how are you?\"\nsample_input = tokenizer_en.encode(sample_sentence)\nsample_input = tf.expand_dims(sample_input, axis=0)\n\n# Prediction\noutput = transformer.predict([sample_input, tf.zeros_like(sample_input)])\noutput_sentence = tokenizer_de.decode([int(i) for i in tf.argmax(output, axis=-1)[0] if i < tokenizer_de.vocab_size])\n\nprint(f\"Input: {sample_sentence}\")\nprint(f\"Translation: {output_sentence}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-18T11:16:34.350822Z","iopub.execute_input":"2024-10-18T11:16:34.351377Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\u001b[1mDownloading and preparing dataset 1.58 GiB (download: 1.58 GiB, generated: Unknown size, total: 1.58 GiB) to /root/tensorflow_datasets/wmt14_translate/de-en/1.0.0...\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Dl Completed...: 0 url [00:00, ? url/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbac743a5c804174894a644036d0b34f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Dl Size...: 0 MiB [00:00, ? MiB/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5565dffe1024491bc46ec40e4ca3263"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extraction completed...: 0 file [00:00, ? file/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cfb28a6c7944f2481867e26bf4686c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Dl Completed...: 0 url [00:00, ? url/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bda5be369c1f478abed8e6c5175d556e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Dl Size...: 0 MiB [00:00, ? MiB/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52c11330dfbc4c01b4d8fb218bd6eb04"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extraction completed...: 0 file [00:00, ? file/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"287e1c05ca3448c28181dd9a6b50f78b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Dl Completed...: 0 url [00:00, ? url/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac065c6a41684badae9d5948b6eb5ecf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Dl Size...: 0 MiB [00:00, ? MiB/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d58acfe073ba4e608d9a7400178100a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extraction completed...: 0 file [00:00, ? file/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae582bbc34934a69865bd823f24c7d2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Dl Completed...: 0 url [00:00, ? url/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"824e9867d08344509760815a16757d18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Dl Size...: 0 MiB [00:00, ? MiB/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b157616a7ce4ca492c22ef82dd0a606"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extraction completed...: 0 file [00:00, ? file/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a25a3b276a9648fabd3e1985d93d6320"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Dl Completed...: 0 url [00:00, ? url/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6325dc846c043d091c609da36bb3bc8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Dl Size...: 0 MiB [00:00, ? MiB/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2607aea1c3b441d39a46307c716655fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extraction completed...: 0 file [00:00, ? file/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f232e089122c4ecf923cbd5bb5ba7059"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Dl Completed...: 0 url [00:00, ? url/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23c301687eee43998bffcdb0f68e76f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Dl Size...: 0 MiB [00:00, ? MiB/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b45e40c545a946ae80553c5d4e94d267"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extraction completed...: 0 file [00:00, ? file/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1dc73c3b4a4b49de892904dc61ab765d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Dl Completed...: 0 url [00:00, ? url/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6399b8a8567a471e9eb215e076e034bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Dl Size...: 0 MiB [00:00, ? MiB/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"831932c00d91430db38be167edba4806"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extraction completed...: 0 file [00:00, ? file/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48186d72f09d4663978228333f5651b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extraction completed...: 0 file [00:00, ? file/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d3c8d0d4a70448c8db6a52a329580dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train examples...:   0%|          | 0/4508785 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Shuffling /root/tensorflow_datasets/wmt14_translate/de-en/incomplete.9YWK6M_1.0.0/wmt14_translate-train.tfreco…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation examples...:   0%|          | 0/3000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Shuffling /root/tensorflow_datasets/wmt14_translate/de-en/incomplete.9YWK6M_1.0.0/wmt14_translate-validation.t…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test examples...:   0%|          | 0/3003 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Shuffling /root/tensorflow_datasets/wmt14_translate/de-en/incomplete.9YWK6M_1.0.0/wmt14_translate-test.tfrecor…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\u001b[1mDataset wmt14_translate downloaded and prepared to /root/tensorflow_datasets/wmt14_translate/de-en/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}