{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1450246,"sourceType":"datasetVersion","datasetId":850118}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"A. BASIC RULE-BASED CHATBOT USING PYTHON NLTK","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# Step 2: Import Libraries\n\nimport nltk\n\nfrom nltk.chat.util import Chat, reflections\n\n\n\n# Step 3: Define Rules (Predefined pairs)\n\npairs = [\n\n    (r\"my name is (.*)\", [\"Hello %1, How are you today?\"]),\n\n    (r\"hi|hey|hello\", [\"Hello\", \"Hey there\"]),\n\n    (r\"what is your name?\", [\"I am a bot .\"]),\n\n    (r\"how are you?\", [\"I'm doing good. \"]),\n\n    (r\"sorry (.*)\", [\"No problem\", \"It's okay\", \"You don't need to be sorry\"]),\n\n    (r\"quit\", [\"Bye! Take care.\"])\n\n]\n\n\n\n# Step 4: Create the Chatbot\n\ndef chatbot():\n\n    print(\"Hi, I'm the chatbot you created. Type 'quit' to exit.\") \n\n    chat = Chat(pairs, reflections)\n\n    chat.converse()\n\n    \n\n# Step 5: Run the Chatbot\n\nif __name__ == \"__main__\":\n\n    chatbot()\n","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Hi, I'm the chatbot you created. Type 'quit' to exit.\n"]},{"name":"stdin","output_type":"stream","text":["> hi\n"]},{"name":"stdout","output_type":"stream","text":["Hello\n"]},{"name":"stdin","output_type":"stream","text":["> how are you\n"]},{"name":"stdout","output_type":"stream","text":["I'm doing good. \n"]},{"name":"stdin","output_type":"stream","text":["> quit\n"]},{"name":"stdout","output_type":"stream","text":["Bye! Take care.\n"]}],"execution_count":2},{"cell_type":"markdown","source":"B) BUILDING A CHATBOT USING SEQ2SEQ MODELS\n\n\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nimport tensorflow as tf\n\nfrom tensorflow.keras.models import Model\n\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n\n\n# Step 1: Load and Preprocess the Dataset\n\ndef load_data(filepath):\n\n    try:\n\n        with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n\n            lines = f.readlines()\n\n    except Exception as e:\n\n        print(f\"Error reading the file: {e}\")\n\n        return []\n\n\n\n    conversations = []\n\n    for line in lines:\n\n        line_parts = line.strip().split(' +++$+++ ')\n\n        if len(line_parts) == 5:\n\n            conversations.append(line_parts[4])  # Store only the dialogue part\n\n\n\n    print(f\"Loaded {len(conversations)} conversations.\")  # Debug info\n\n    return conversations\n\n\n\ndef create_pairs(conversations):\n\n    input_texts = []\n\n    target_texts = []\n\n\n\n    for i in range(len(conversations) - 1):\n\n        input_text = conversations[i]\n\n        target_text = conversations[i + 1]\n\n        target_text = '\\t' + target_text + '\\n'  # Add start and end tokens\n\n        input_texts.append(input_text)\n\n        target_texts.append(target_text)\n\n\n\n    print(f\"Created {len(input_texts)} input-target pairs.\")  # Debug info\n\n    return input_texts, target_texts\n\n\n\n# Load the dataset (replace with the correct path to movie_lines.txt)\n\nconversations = load_data('/kaggle/input/movie-dialogs/movie_lines.txt')  # Make sure this file exists\n\ninput_texts, target_texts = create_pairs(conversations)\n\n\n\n# Check if input_texts and target_texts are populated\n\nif not input_texts or not target_texts:\n\n    raise ValueError(\"No input or target texts were created. Please check the dataset.\")\n\n\n\n# Step 2: Tokenize and Pad the Data\n\n# Tokenize the input and output data\n\ninput_tokenizer = Tokenizer()\n\ntarget_tokenizer = Tokenizer()\n\n\n\ninput_tokenizer.fit_on_texts(input_texts)\n\ntarget_tokenizer.fit_on_texts(target_texts)\n\n\n\ninput_sequences = input_tokenizer.texts_to_sequences(input_texts)\n\ntarget_sequences = target_tokenizer.texts_to_sequences(target_texts)\n\n\n\n# Pad sequences to ensure uniform length\n\nmax_encoder_seq_length = max(len(seq) for seq in input_sequences) if input_sequences else 0\n\nmax_decoder_seq_length = max(len(seq) for seq in target_sequences) if target_sequences else 0\n\n\n\nencoder_input_data = pad_sequences(input_sequences, maxlen=max_encoder_seq_length, padding='post')\n\ndecoder_input_data = pad_sequences(target_sequences, maxlen=max_decoder_seq_length, padding='post')\n\n\n\n# Prepare decoder output data\n\ndecoder_output_data = np.zeros((len(target_sequences), max_decoder_seq_length, len(target_tokenizer.word_index) + 1), dtype='float32')\n\n\n\nfor i, seq in enumerate(target_sequences):\n\n    for t, word_idx in enumerate(seq):\n\n        if t > 0:\n\n            decoder_output_data[i, t - 1, word_idx] = 1.0\n\n\n\n# Step 3: Build the Seq2Seq Model\n\nnum_encoder_tokens = len(input_tokenizer.word_index) + 1\n\nnum_decoder_tokens = len(target_tokenizer.word_index) + 1\n\n\n\n# Encoder\n\nencoder_inputs = Input(shape=(None,))\n\nencoder_embedding = Embedding(input_dim=num_encoder_tokens, output_dim=256)(encoder_inputs)\n\nencoder_lstm = LSTM(256, return_state=True)\n\nencoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n\n\n\n# Save the encoder states to pass to the decoder\n\nencoder_states = [state_h, state_c]\n\n\n\n# Decoder\n\ndecoder_inputs = Input(shape=(None,))\n\ndecoder_embedding = Embedding(input_dim=num_decoder_tokens, output_dim=256)(decoder_inputs)\n\ndecoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n\ndecoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n\ndecoder_dense = Dense(num_decoder_tokens, activation='softmax')\n\ndecoder_outputs = decoder_dense(decoder_outputs)\n\n\n\n# Define the model\n\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n\n\n\n# Step 4: Compile and Train the Model\n\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n\n\n\n# Train the model (adjust epochs and batch size as needed)\n\nmodel.fit([encoder_input_data, decoder_input_data], decoder_output_data, batch_size=64, epochs=100)\n\n\n\n# Step 5: Inference Setup (for generating responses)\n\nencoder_model = Model(encoder_inputs, encoder_states)\n\n\n\n# Decoder setup\n\ndecoder_state_input_h = Input(shape=(256,))\n\ndecoder_state_input_c = Input(shape=(256,))\n\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n\n\n\ndecoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, initial_state=decoder_states_inputs)\n\ndecoder_states = [state_h, state_c]\n\ndecoder_outputs = decoder_dense(decoder_outputs)\n\n\n\ndecoder_model = Model(\n\n    [decoder_inputs] + decoder_states_inputs,\n\n    [decoder_outputs] + decoder_states)\n\n\n\n# Step 6: Decode a Sequence (Generate a Response)\n\ndef decode_sequence(input_seq):\n\n    # Encode the input as state vectors\n\n    states_value = encoder_model.predict(input_seq)\n\n\n\n    # Generate an empty target sequence with only the start token\n\n    target_seq = np.zeros((1, 1))\n\n    target_seq[0, 0] = target_tokenizer.word_index['\\t']\n\n\n\n    stop_condition = False\n\n    decoded_sentence = ''\n\n\n\n    while not stop_condition:\n\n        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n\n\n\n        # Sample the next token\n\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n\n        sampled_char = target_tokenizer.index_word.get(sampled_token_index, '')\n\n        decoded_sentence += sampled_char\n\n\n\n        if (sampled_char == '\\n' or len(decoded_sentence) > max_decoder_seq_length):\n\n            stop_condition = True\n\n\n\n        # Update the target sequence and states\n\n        target_seq = np.zeros((1, 1))\n\n        target_seq[0, 0] = sampled_token_index\n\n        states_value = [h, c]\n\n\n\n    return decoded_sentence.strip()  # Trim any extra whitespace\n\n\n\n# Step 7: Test the Chatbot\n\ndef chat():\n\n    print(\"Chatbot is ready! Type 'quit' to exit.\")\n\n    while True:\n\n        input_text = input(\"You: \")\n\n        if input_text.lower() == 'quit':\n\n            print(\"Exiting the chat. Goodbye!\")\n\n            break\n\n\n\n        input_sequence = input_tokenizer.texts_to_sequences([input_text])\n\n        input_sequence = pad_sequences(input_sequence, maxlen=max_encoder_seq_length, padding='post')\n\n        response = decode_sequence(input_sequence)\n\n        print(f\"Bot: {response}\")\n\n\n\nif __name__ == \"__main__\":\n\n    chat()\n","metadata":{"execution":{"iopub.execute_input":"2024-10-06T05:38:41.575203Z","iopub.status.busy":"2024-10-06T05:38:41.574709Z"}},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded 304446 conversations.\n","Created 304445 input-target pairs.\n"]}],"execution_count":null},{"cell_type":"code","source":"C. CONVERSATIONAL AI WITH TRANSFORMER-BASED MODELS\n\n\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import necessary libraries\n\nimport pandas as pd\n\nimport torch\n\nfrom transformers import AutoTokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n\n\n\n# Load your dataset (example with a CSV file)\n\ndata = pd.read_csv('path_to_your_dataset.csv')  # Replace with your dataset path\n\nconversations = data[['input', 'output']]  # Adjust column names based on your dataset\n\n\n\n# Initialize the tokenizer and model (using GPT-2 for this example)\n\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n\n\n\n# Data Preprocessing\n\ndef preprocess_data(conversations):\n\n    inputs = [\"Question: \" + q for q in conversations['input'].tolist()]\n\n    outputs = [\"Answer: \" + a for a in conversations['output'].tolist()]\n\n    return inputs, outputs\n\n\n\ninputs, outputs = preprocess_data(conversations)\n\n\n\n# Create input-output pairs for training\n\ntrain_data = list(zip(inputs, outputs))\n\n\n\n# Tokenization\n\ntrain_encodings = tokenizer(inputs, truncation=True, padding=True, max_length=50)\n\ntrain_labels = tokenizer(outputs, truncation=True, padding=True, max_length=50)\n\n\n\n# Prepare dataset for Trainer\n\nclass ChatDataset(torch.utils.data.Dataset):\n\n    def __init__(self, encodings, labels):\n\n        self.encodings = encodings\n\n        self.labels = labels\n\n\n\n    def __getitem__(self, idx):\n\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n\n        item['labels'] = torch.tensor(self.labels['input_ids'][idx])\n\n        return item\n\n\n\n    def __len__(self):\n\n        return len(self.labels['input_ids'])\n\n\n\n# Create dataset object\n\ntrain_dataset = ChatDataset(train_encodings, train_labels)\n\n\n\n# Training arguments\n\ntraining_args = TrainingArguments(\n\n    output_dir='./results',\n\n    num_train_epochs=3,\n\n    per_device_train_batch_size=8,\n\n    logging_dir='./logs',\n\n    logging_steps=10,\n\n)\n\n\n\n# Initialize Trainer\n\ntrainer = Trainer(\n\n    model=model,\n\n    args=training_args,\n\n    train_dataset=train_dataset,\n\n)\n\n\n\n# Train the model\n\ntrainer.train()\n\n\n\n# Generating responses\n\ndef generate_response(question):\n\n    input_ids = tokenizer.encode(\"Question: \" + question, return_tensors='pt')\n\n    output = model.generate(input_ids, max_length=50, num_return_sequences=1)\n\n    response = tokenizer.decode(output[0], skip_special_tokens=True)\n\n    return response.replace(\"Question: \", \"\").replace(\"Answer: \", \"\").strip()\n\n\n\n# Sample Input and Output\n\nsample_input = \"What's the weather like today?\"\n\nexpected_output = generate_response(sample_input)\n\n\n\nprint(f\"Input: {sample_input}\")\n\nprint(f\"Expected Output: {expected_output}\")\n","metadata":{},"outputs":[],"execution_count":null}]}