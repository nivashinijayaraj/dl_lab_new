{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8482,"sourceType":"datasetVersion","datasetId":5689},{"sourceId":163621,"sourceType":"datasetVersion","datasetId":73247},{"sourceId":2020792,"sourceType":"datasetVersion","datasetId":1209510},{"sourceId":2510329,"sourceType":"datasetVersion","datasetId":1520310},{"sourceId":9478143,"sourceType":"datasetVersion","datasetId":5764856},{"sourceId":2859008,"sourceType":"kernelVersion"},{"sourceId":55281833,"sourceType":"kernelVersion"}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"A)Basic Sentiment Analysis using Logistic Regressing ","metadata":{}},{"cell_type":"code","source":"#a\n\n# Import necessary libraries\n\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.metrics import classification_report, accuracy_score\n\nimport re\n\nimport string\n\n\n\n# Step 1: Load the dataset\n\n# Replace 'path_to_dataset/reviews.csv' with the actual path to the IMDb dataset.\n\ndf = pd.read_csv('/kaggle/input/movie-review/labelled_full_dataset.csv')\n\n\n\n# Check the structure of the dataframe\n\nprint(df.head())\n\n\n\n# Step 2: Data Preprocessing\n\n# Clean text function\n\ndef clean_text(text):\n\n    # Remove punctuation\n\n    text = text.translate(str.maketrans('', '', string.punctuation))\n\n    # Convert to lowercase\n\n    text = text.lower()\n\n    # Remove digits\n\n    text = re.sub(r'\\d+', '', text)\n\n    return text\n\n\n\n# Apply cleaning to the 'review' column\n\ndf['cleaned_text'] = df['review'].apply(clean_text)\n\n\n\n# Step 3: Convert text data into numerical vectors\n\nvectorizer = TfidfVectorizer(max_features=5000)  # Use TF-IDF for vectorization\n\nX = vectorizer.fit_transform(df['cleaned_text'])\n\ny = df['label']  # Use the 'label' column for sentiment labels\n\n\n\n# Step 4: Split data into training and testing sets\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n\n# Step 5: Build and train the logistic regression model\n\nmodel = LogisticRegression(max_iter=1000)\n\nmodel.fit(X_train, y_train)\n\n\n\n# Step 6: Evaluate the model\n\ny_pred = model.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\n\nprint(f'Accuracy: {accuracy:.2f}')\n\nprint(classification_report(y_test, y_pred))\n\n\n\n# Sample Input (Review Text)\n\nsample_review = \"This movie was amazing! I loved every minute of it.\"\n\nsample_cleaned = clean_text(sample_review)\n\nsample_vectorized = vectorizer.transform([sample_cleaned])\n\n\n\n# Predict sentiment for the sample input\n\nsample_prediction = model.predict(sample_vectorized)\n\nprint(f'Sample Review Sentiment: {\"Positive\" if sample_prediction[0] == 1 else \"Negative\"}')\n","metadata":{"execution":{"iopub.execute_input":"2024-09-25T12:15:33.112490Z","iopub.status.busy":"2024-09-25T12:15:33.111749Z","iopub.status.idle":"2024-09-25T12:15:49.447495Z","shell.execute_reply":"2024-09-25T12:15:49.446351Z","shell.execute_reply.started":"2024-09-25T12:15:33.112432Z"}},"outputs":[{"name":"stdout","output_type":"stream","text":["   label                                             review\n","0      0  Once again Mr. Costner has dragged out a movie...\n","1      0  This is an example of why the majority of acti...\n","2      0  First of all I hate those moronic rappers, who...\n","3      0  Not even the Beatles could write songs everyon...\n","4      0  Brass pictures (movies is not a fitting word f...\n","Accuracy: 0.89\n","              precision    recall  f1-score   support\n","\n","           0       0.90      0.88      0.89      5022\n","           1       0.88      0.90      0.89      4978\n","\n","    accuracy                           0.89     10000\n","   macro avg       0.89      0.89      0.89     10000\n","weighted avg       0.89      0.89      0.89     10000\n","\n","Sample Review Sentiment: Positive\n"]}],"execution_count":3},{"cell_type":"markdown","source":"B) Twitter Sentiment Analysis Using LSTM and Glove","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\n\nimport pandas as pd\n\nimport numpy as np\n\nimport re\n\nimport string\n\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nfrom tensorflow.keras.models import Sequential\n\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n\n\n# Step 1: Load and preprocess the dataset\n\n# Load the dataset\n\ndf = pd.read_csv('/kaggle/input/twitter-sentiment/Sentiment Analysis Dataset 2.csv', on_bad_lines='skip')\n\n\n\n# Check the structure of the dataframe\n\nprint(df.head())\n\n\n\n# Clean text function\n\ndef clean_text(text):\n\n    # Remove URLs, mentions, hashtags, and special characters\n\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n\n    text = re.sub(r'@\\w+', '', text)\n\n    text = re.sub(r'#', '', text)\n\n    text = text.translate(str.maketrans('', '', string.punctuation))\n\n    text = text.lower()\n\n    return text\n\n\n\n# Apply cleaning to the 'SentimentText' column\n\ndf['cleaned_text'] = df['SentimentText'].apply(clean_text)\n\n\n\n# Convert sentiment labels to numerical format\n\ndf['Sentiment'] = df['Sentiment'].replace({0: 0, 2: 1, 4: 2})  # Adjust based on your labeling scheme\n\n\n\n# Step 2: Tokenize text and pad sequences\n\nmax_length = 100  # Maximum length of sequences\n\ntokenizer = Tokenizer(num_words=5000)\n\ntokenizer.fit_on_texts(df['cleaned_text'])\n\nsequences = tokenizer.texts_to_sequences(df['cleaned_text'])\n\npadded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n\n\n\n# Step 3: Split data into training, validation, and testing sets\n\nX = padded_sequences\n\ny = df['Sentiment'].values\n\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)  # Split the temp set into validation and test sets\n\n\n\n# Step 4: Load pre-trained GloVe embeddings\n\nembeddings_index = {}\n\nglove_file = '/kaggle/input/glove-embeddings/glove.6B.100d.txt'  # Adjust the path to your GloVe file\n\nwith open(glove_file, 'r', encoding='utf-8') as f:\n\n    for line in f:\n\n        values = line.split()\n\n        word = values[0]\n\n        coefs = np.asarray(values[1:], dtype='float32')\n\n        embeddings_index[word] = coefs\n\n\n\n# Create embedding matrix\n\nembedding_dim = 100\n\nembedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\n\nfor word, i in tokenizer.word_index.items():\n\n    embedding_vector = embeddings_index.get(word)\n\n    if embedding_vector is not None:\n\n        embedding_matrix[i] = embedding_vector\n\n\n\n# Step 5: Build LSTM model with GloVe embeddings\n\nmodel = Sequential()\n\nmodel.add(Embedding(input_dim=len(tokenizer.word_index) + 1, \n\n                    output_dim=embedding_dim, \n\n                    weights=[embedding_matrix], \n\n                    input_length=max_length, \n\n                    trainable=False))\n\nmodel.add(LSTM(100, return_sequences=True))\n\nmodel.add(Dropout(0.5))\n\nmodel.add(LSTM(100))\n\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(3, activation='softmax'))  # 3 classes (0, 1, 2)\n\n\n\n# Compile the model\n\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n\n\n# Step 6: Train the model with validation data\n\nearly_stopping = EarlyStopping(monitor='val_loss', patience=3)\n\nhistory = model.fit(X_train, y_train, epochs=3, batch_size=64, \n\n                    validation_data=(X_val, y_val), callbacks=[early_stopping])\n\n\n\n# Step 7: Evaluate the model\n\nloss, accuracy = model.evaluate(X_test, y_test)\n\nprint(f'Test Accuracy: {accuracy:.2f}')\n\n\n\n# Sample Input (Tweet Text)\n\nsample_tweet = \"I really enjoyed the movie, it was fantastic!\"\n\nsample_cleaned = clean_text(sample_tweet)\n\nsample_sequence = tokenizer.texts_to_sequences([sample_cleaned])\n\nsample_padded = pad_sequences(sample_sequence, maxlen=max_length, padding='post')\n\n\n\n# Predict sentiment for the sample input\n\nsample_prediction = model.predict(sample_padded)\n\nprint(f'Sample Tweet Sentiment: {np.argmax(sample_prediction)}')  # Output the sentiment class\n","metadata":{"execution":{"iopub.execute_input":"2024-09-25T12:16:07.198550Z","iopub.status.busy":"2024-09-25T12:16:07.197753Z"}},"outputs":[{"name":"stdout","output_type":"stream","text":["   ItemID  Sentiment SentimentSource  \\\n","0       1          0    Sentiment140   \n","1       2          0    Sentiment140   \n","2       3          1    Sentiment140   \n","3       4          0    Sentiment140   \n","4       5          0    Sentiment140   \n","\n","                                       SentimentText  \n","0                       is so sad for my APL frie...  \n","1                     I missed the New Moon trail...  \n","2                            omg its already 7:30 :O  \n","3            .. Omgaga. Im sooo  im gunna CRy. I'...  \n","4           i think mi bf is cheating on me!!!   ...  \n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/3\n","\u001b[1m19111/19733\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2:02\u001b[0m 197ms/step - accuracy: 0.4987 - loss: 0.6969"]}],"execution_count":null},{"cell_type":"code","source":"import os\n\nprint(os.listdir('/kaggle/input/'))","metadata":{"execution":{"iopub.execute_input":"2024-09-25T10:13:48.923631Z","iopub.status.busy":"2024-09-25T10:13:48.923145Z","iopub.status.idle":"2024-09-25T10:13:48.930518Z","shell.execute_reply":"2024-09-25T10:13:48.929168Z","shell.execute_reply.started":"2024-09-25T10:13:48.923590Z"}},"outputs":[{"name":"stdout","output_type":"stream","text":["['using-word-embeddings-for-sentiment-analysis', 'imdb-review', 'twitter-sentiment', 'movie-review', 'twitter-entity-sentiment-analysis', 'd', 'twitter-sentiment-analysis-using-tensorflow']\n"]}],"execution_count":5},{"cell_type":"markdown","source":"C) Movie Reviews Sentiment Classification with Bert ","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\n\nimport pandas as pd\n\nimport numpy as np\n\nimport torch\n\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import BertTokenizer, BertForSequenceClassification\n\nfrom transformers import AdamW\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import accuracy_score, classification_report\n\n\n\n# Step 1: Load and preprocess the dataset\n\n# Load the dataset \n\ndf = pd.read_csv('/kaggle/input/imdb-review/imdb_reviews.csv')  # Adjust to your actual dataset path\n\n\n\n# Display the first few rows of the dataset\n\nprint(df.head())\n\n\n\n# Convert sentiment labels to numerical format (0 for negative, 1 for positive)\n\ndf['sentiment'] = df['sentiment'].map({'neg': 0, 'pos': 1})\n\n\n\n# Clean text function (optional, based on dataset specifics)\n\ndef clean_text(text):\n\n    # Here, you can add any cleaning steps if necessary (e.g., removing special characters)\n\n    return text\n\n\n\n# Apply text cleaning\n\ndf['text'] = df['text'].apply(clean_text)\n\n\n\n# Step 2: Tokenize and encode reviews using BERT tokenizer\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n\n\nclass IMDBDataset(Dataset):\n\n    def __init__(self, reviews, labels):\n\n        self.reviews = reviews\n\n        self.labels = labels\n\n\n\n    def __len__(self):\n\n        return len(self.reviews)\n\n\n\n    def __getitem__(self, idx):\n\n        review = self.reviews[idx]\n\n        label = self.labels[idx]\n\n\n\n        # Tokenize and encode the review\n\n        encoding = tokenizer.encode_plus(\n\n            review,\n\n            add_special_tokens=True,\n\n            max_length=256,\n\n            return_token_type_ids=False,\n\n            padding='max_length',\n\n            truncation=True,\n\n            return_attention_mask=True,\n\n            return_tensors='pt',\n\n        )\n\n\n\n        return {\n\n            'input_ids': encoding['input_ids'].flatten(),\n\n            'attention_mask': encoding['attention_mask'].flatten(),\n\n            'labels': torch.tensor(label, dtype=torch.long)\n\n        }\n\n\n\n# Step 3: Split data into training and testing sets\n\nX_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.2, random_state=42)\n\n\n\n# Create datasets\n\ntrain_dataset = IMDBDataset(X_train.to_numpy(), y_train.to_numpy())\n\ntest_dataset = IMDBDataset(X_test.to_numpy(), y_test.to_numpy())\n\n\n\n# Create data loaders\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n\n\n\n# Step 4: Load pre-trained BERT model and fine-tune for sentiment classification\n\n# Load tokenizer from a local directory where BERT files are stored\n\ntokenizer = BertTokenizer.from_pretrained('/kaggle/input/bert-base-uncased')\n\nmodel = BertForSequenceClassification.from_pretrained('/kaggle/input/bert-base-uncased', num_labels=2)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel = model.to(device)\n\n\n\n# Set up the optimizer\n\noptimizer = AdamW(model.parameters(), lr=2e-5)\n\n\n\n# Train the model\n\nmodel.train()\n\nfor epoch in range(3):  # Adjust number of epochs as necessary\n\n    total_loss = 0\n\n    for batch in train_loader:\n\n        optimizer.zero_grad()\n\n        input_ids = batch['input_ids'].to(device)\n\n        attention_mask = batch['attention_mask'].to(device)\n\n        labels = batch['labels'].to(device)\n\n\n\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n\n        loss = outputs.loss\n\n        total_loss += loss.item()\n\n        loss.backward()\n\n        optimizer.step()\n\n\n\n    print(f'Epoch {epoch + 1}/{3}, Loss: {total_loss / len(train_loader)}')\n\n\n\n# Step 5: Evaluate the model\n\nmodel.eval()\n\npredictions, true_labels = [], []\n\n\n\nwith torch.no_grad():\n\n    for batch in test_loader:\n\n        input_ids = batch['input_ids'].to(device)\n\n        attention_mask = batch['attention_mask'].to(device)\n\n\n\n        outputs = model(input_ids, attention_mask=attention_mask)\n\n        logits = outputs.logits\n\n        preds = torch.argmax(logits, dim=1).cpu().numpy()\n\n\n\n        predictions.extend(preds)\n\n        true_labels.extend(batch['labels'].cpu().numpy())\n\n\n\n# Calculate accuracy and classification report\n\naccuracy = accuracy_score(true_labels, predictions)\n\nprint(f'Accuracy: {accuracy}')\n\nprint(classification_report(true_labels, predictions, target_names=['Negative', 'Positive']))\n\n\n\n# Sample Input\n\nsample_review = \"The movie was boring and uninteresting.\"\n\nencoded_sample = tokenizer.encode_plus(\n\n    sample_review,\n\n    add_special_tokens=True,\n\n    max_length=256,\n\n    return_token_type_ids=False,\n\n    padding='max_length',\n\n    truncation=True,\n\n    return_attention_mask=True,\n\n    return_tensors='pt',\n\n)\n\n\n\n# Make prediction on the sample input\n\nmodel.eval()\n\nwith torch.no_grad():\n\n    input_ids = encoded_sample['input_ids'].to(device)\n\n    attention_mask = encoded_sample['attention_mask'].to(device)\n\n    output = model(input_ids, attention_mask=attention_mask)\n\n    prediction = torch.argmax(output.logits, dim=1).cpu().numpy()\n\n\n\nprint(f'Sample Input: \"{sample_review}\"')\n\nprint(f'Expected Output: {\"Positive\" if prediction[0] == 1 else \"Negative\"}')\n","metadata":{"execution":{"iopub.status.busy":"2024-09-25T11:16:48.115977Z","iopub.status.idle":"2024-09-25T11:16:48.116440Z","shell.execute_reply":"2024-09-25T11:16:48.116248Z","shell.execute_reply.started":"2024-09-25T11:16:48.116225Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}